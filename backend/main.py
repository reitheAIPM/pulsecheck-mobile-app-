#!/usr/bin/env python3
"""
PulseCheck API - AI-powered burnout prevention for tech workers
FastAPI backend with Supabase integration and OpenAI-powered insights

Version: 2.1.2-cors-fix-v3
Last Updated: 2025-01-25 - Enhanced CORS handling for Vercel domains
"""

# DEPLOYMENT TRIGGER: Force Railway rebuild - AI RLS BYPASS FIX v1.3 CRITICAL DEPLOYMENT
# This comment forces Railway to rebuild container with SERVICE ROLE CLIENT for AI
# Deployment timestamp: 2025-06-29 05:10 UTC - CRITICAL FIX: DNS RESOLUTION ISSUE

from fastapi import FastAPI, Request, HTTPException
from fastapi.middleware.cors import CORSMiddleware
from fastapi.middleware.trustedhost import TrustedHostMiddleware
from fastapi.middleware.gzip import GZipMiddleware
from fastapi.responses import JSONResponse, Response
import time
import logging
from contextlib import asynccontextmanager
import os
from dotenv import load_dotenv
from starlette.middleware.base import BaseHTTPMiddleware
from typing import Dict, Any
from datetime import datetime, timezone
import sys
import traceback
import asyncio
from starlette.types import ASGIApp, Receive, Scope, Send
from starlette.datastructures import MutableHeaders
import re
import signal

# Load environment variables
load_dotenv()

# FORCE IMMEDIATE STARTUP LOGGING
print("üöÄ PulseCheck v2.0.0 with Enhanced Debug Logging - STARTING UP!")
print("üöÄ This should appear in Railway logs immediately!")
sys.stdout.flush()

# CRITICAL: System health checks before any imports
print("üîç Running critical system health checks...")
sys.stdout.flush()

# 1. Python Runtime Checks
try:
    import sys
    print(f"‚úÖ Python version: {sys.version}")
    print(f"‚úÖ Python path: {sys.executable}")
    print(f"‚úÖ Recursion limit: {sys.getrecursionlimit()}")
    
    # Check memory usage
    import psutil
    memory_info = psutil.virtual_memory()
    print(f"‚úÖ Available memory: {memory_info.available / (1024**3):.2f} GB")
    print(f"‚úÖ Memory usage: {memory_info.percent}%")
except Exception as e:
    print(f"‚ö†Ô∏è Memory check failed: {e}")

# 2. Network Connectivity Checks
try:
    import socket
    import requests
    
    # Check DNS resolution
    try:
        socket.gethostbyname('google.com')
        print("‚úÖ DNS resolution working")
    except Exception as e:
        print(f"‚ö†Ô∏è DNS resolution failed: {e}")
    
    # Check network connectivity
    try:
        response = requests.get("https://httpbin.org/get", timeout=5)
        print("‚úÖ Network connectivity working")
    except Exception as e:
        print(f"‚ö†Ô∏è Network connectivity failed: {e}")
        
except Exception as e:
    print(f"‚ö†Ô∏è Network checks failed: {e}")

# 3. Package Availability Checks
required_packages = [
    'fastapi', 'uvicorn', 'pydantic', 'supabase',
    'openai', 'python-dotenv', 'slowapi', 'requests'
]

missing_packages = []
for package in required_packages:
    try:
        __import__(package)
        print(f"‚úÖ Package available: {package}")
    except ImportError:
        missing_packages.append(package)
        print(f"‚ùå Missing package: {package}")

if missing_packages:
    print(f"‚ö†Ô∏è WARNING: Missing packages: {missing_packages}")
    print("‚ö†Ô∏è Application may not function properly without these packages")
else:
    print("‚úÖ All required packages available")

# 4. Port Availability Check
try:
    import socket
    def check_port_availability(port=8000):
        try:
            with socket.socket(socket.AF_INET, socket.SOCK_STREAM) as s:
                s.bind(('localhost', port))
                return True
        except:
            return False
    
    if check_port_availability():
        print("‚úÖ Port 8000 available")
    else:
        print("‚ö†Ô∏è Port 8000 may be in use")
except Exception as e:
    print(f"‚ö†Ô∏è Port check failed: {e}")

# 5. Environment Variable Validation
print("üîç Validating essential environment variables...")
sys.stdout.flush()

critical_env_vars = {
    "SUPABASE_URL": os.getenv("SUPABASE_URL"),
    "SUPABASE_ANON_KEY": os.getenv("SUPABASE_ANON_KEY"),
    "OPENAI_API_KEY": os.getenv("OPENAI_API_KEY"),
}

missing_critical_vars = []
for var_name, var_value in critical_env_vars.items():
    if not var_value:
        missing_critical_vars.append(var_name)
        print(f"‚ùå Missing critical environment variable: {var_name}")
    else:
        print(f"‚úÖ Found environment variable: {var_name}")

if missing_critical_vars:
    print(f"‚ö†Ô∏è WARNING: Missing critical environment variables: {missing_critical_vars}")
    print("‚ö†Ô∏è Application may not function properly without these variables")
    print("‚ö†Ô∏è Health checks will still work, but features may be limited")
else:
    print("‚úÖ All critical environment variables found")

# 6. Railway-Specific Checks
railway_env_vars = {
    "RAILWAY_ENVIRONMENT": os.getenv("RAILWAY_ENVIRONMENT"),
    "RAILWAY_PROJECT_ID": os.getenv("RAILWAY_PROJECT_ID"),
    "PORT": os.getenv("PORT"),
}

print("üîç Railway environment check...")
for var_name, var_value in railway_env_vars.items():
    if var_value:
        print(f"‚úÖ Railway variable: {var_name}")
    else:
        print(f"‚ö†Ô∏è Missing Railway variable: {var_name}")

# 7. Setup startup timeout for Railway
try:
    import signal
    def setup_startup_timeout():
        def timeout_handler(signum, frame):
            print("‚è∞ Startup timeout - exiting gracefully")
            sys.exit(1)
        signal.signal(signal.SIGALRM, timeout_handler)
        signal.alarm(300)  # 5 minute timeout for Railway
    
    setup_startup_timeout()
    print("‚úÖ Startup timeout configured (5 minutes)")
except Exception as e:
    print(f"‚ö†Ô∏è Startup timeout setup failed: {e}")

# 8. Startup Failure Recovery
def check_startup_viability():
    """Check if startup should proceed or if we should exit gracefully"""
    critical_failures = []
    
    # Check if we have minimum required packages
    min_required_packages = ['fastapi', 'uvicorn']
    for package in min_required_packages:
        try:
            __import__(package)
        except ImportError:
            critical_failures.append(f"Missing critical package: {package}")
    
    # Check if we have minimum memory
    try:
        import psutil
        memory_info = psutil.virtual_memory()
        if memory_info.available < 100 * 1024 * 1024:  # Less than 100MB
            critical_failures.append("Insufficient memory available")
    except:
        pass  # Skip memory check if psutil not available
    
    # Check if we can bind to port
    try:
        import socket
        with socket.socket(socket.AF_INET, socket.SOCK_STREAM) as s:
            s.bind(('localhost', 8000))
    except:
        critical_failures.append("Cannot bind to port 8000")
    
    if critical_failures:
        print("‚ùå CRITICAL STARTUP FAILURES DETECTED:")
        for failure in critical_failures:
            print(f"   - {failure}")
        print("‚ùå Exiting gracefully to prevent deployment issues")
        sys.exit(1)
    else:
        print("‚úÖ Startup viability check passed")

# Run startup viability check
check_startup_viability()

print("üîç System health checks complete!")
sys.stdout.flush()

# Import DNS helper for Railway
try:
    from app.core.dns_helper import configure_dns
    print("üîß Configuring DNS for Railway environment...")
    configure_dns()
except Exception as e:
    print(f"‚ö†Ô∏è DNS helper not available: {e}")

# Configure logging early
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

# Import configuration and routers with comprehensive error handling
try:
    from app.core.config import settings
    from app.core.security import setup_rate_limiting, limiter
    config_loaded = True
    print("‚úÖ Core configuration loaded successfully")
except Exception as e:
    print(f"‚ùå Configuration loading failed: {e}")
    config_loaded = False
    # Create minimal settings for health checks
    class MinimalSettings:
        environment = "production"
        allowed_origins_list = ["*"]
        host = "0.0.0.0"
        port = 8000
        def validate_required_settings(self):
            pass  # Skip validation for minimal settings
    settings = MinimalSettings()
    limiter = None

# Import monitoring with error handling
try:
    from app.core.monitoring import monitor, log_error, log_performance, check_health, ErrorSeverity, ErrorCategory
    monitoring_loaded = True
    print("‚úÖ Monitoring system loaded successfully")
except Exception as e:
    print(f"‚ùå Monitoring system failed to load: {e}")
    monitoring_loaded = False
    # Create minimal monitoring fallback
    class MinimalMonitor:
        def check_system_health(self):
            return type('obj', (object,), {'overall_status': 'unknown'})()
    monitor = MinimalMonitor()
    def log_error(*args, **kwargs):
        print(f"ERROR: {args}")
    def log_performance(*args, **kwargs):
        pass
    ErrorSeverity = type('obj', (object,), {'HIGH': 'high', 'MEDIUM': 'medium', 'LOW': 'low'})()
    ErrorCategory = type('obj', (object,), {'API_ENDPOINT': 'api_endpoint'})()
    def check_health():
        return type('obj', (object,), {'overall_status': 'unknown'})()

# Import database with error handling
try:
    from app.core.database import engine, Base, get_database
    database_loaded = True
    print("‚úÖ Database system loaded successfully")
except Exception as e:
    print(f"‚ùå Database system failed to load: {e}")
    database_loaded = False
    def get_database():
        raise Exception("Database system not available")

# Import observability with error handling
try:
    from app.core.observability import init_observability, observability
    observability_loaded = True
    print("‚úÖ Observability system loaded successfully")
except Exception as e:
    print(f"‚ùå Observability system failed to load: {e}")
    observability_loaded = False
    def init_observability():
        print("Observability initialization skipped")
    observability = type('obj', (object,), {
        'capture_error': lambda *args, **kwargs: None,
        'get_ai_debugging_summary': lambda: {'status': 'not_available'}
    })()

# Import middleware with error handling
try:
    from app.middleware.observability_middleware import ObservabilityMiddleware
    from app.middleware.security_headers import SecurityHeadersMiddleware
    middleware_loaded = True
    print("‚úÖ Middleware systems loaded successfully")
except Exception as e:
    print(f"‚ùå Middleware systems failed to load: {e}")
    middleware_loaded = False
    # Create minimal middleware fallbacks
    class MinimalObservabilityMiddleware:
        def __init__(self, app):
            self.app = app
        async def __call__(self, scope, receive, send):
            await self.app(scope, receive, send)
    
    class MinimalSecurityHeadersMiddleware:
        def __init__(self, app):
            self.app = app
        async def __call__(self, scope, receive, send):
            await self.app(scope, receive, send)
    
    ObservabilityMiddleware = MinimalObservabilityMiddleware
    SecurityHeadersMiddleware = MinimalSecurityHeadersMiddleware

# Import required modules for lifespan and services
from app.core.database import get_database, init_supabase
from app.middleware.observability_middleware import ObservabilityMiddleware

# Import scheduler service with error handling for Railway deployment
try:
    from app.services.advanced_scheduler_service import get_scheduler_service
    scheduler_available = True
except ImportError as e:
    logger.warning(f"Scheduler service not available: {e}")
    scheduler_available = False
    
    def get_scheduler_service():
        raise ImportError("Scheduler service not available")

# Global scheduler reference for graceful shutdown
scheduler_service = None

async def monitor_scheduler_health():
    """Background task to monitor and restart scheduler if it stops"""
    global scheduler_service
    
    # Wait for initial startup to complete
    await asyncio.sleep(30)
    
    consecutive_failures = 0
    
    while True:
        try:
            await asyncio.sleep(60)  # Check every minute
            
            if scheduler_service and scheduler_available:
                status = scheduler_service.get_scheduler_status()
                
                # Check if scheduler is not running
                if status.get("status") != "running" or not status.get("running", False):
                    consecutive_failures += 1
                    logger.warning(f"üö® Scheduler detected as stopped (attempt {consecutive_failures}), attempting restart...")
                    
                    try:
                        # Stop first to clean up any partial state
                        try:
                            await scheduler_service.stop_scheduler()
                        except:
                            pass
                        
                        # Wait a moment
                        await asyncio.sleep(2)
                        
                        # Start the scheduler
                        result = await scheduler_service.start_scheduler()
                        
                        if result.get("status") == "started":
                            logger.info("‚úÖ Scheduler auto-recovery successful")
                            consecutive_failures = 0
                            
                            # Enable testing mode if configured
                            if os.getenv("ENABLE_TESTING_MODE", "true").lower() == "true":
                                try:
                                    scheduler_service.proactive_ai.enable_testing_mode()
                                    logger.info("üß™ Testing mode auto-enabled after recovery")
                                except:
                                    pass
                        else:
                            logger.error(f"‚ùå Scheduler auto-recovery failed: {result}")
                            
                            # If we've failed 3 times, try a full restart
                            if consecutive_failures >= 3:
                                logger.error("üîÑ Attempting full scheduler service recreation...")
                                scheduler_service = get_scheduler_service()
                                await asyncio.sleep(5)
                    except Exception as e:
                        logger.error(f"‚ùå Scheduler auto-recovery error: {e}")
                else:
                    # Scheduler is running fine
                    consecutive_failures = 0
                    
                    # Log health check every 5 minutes
                    if int(time.time()) % 300 < 60:
                        metrics = status.get("metrics", {})
                        logger.info(f"‚úÖ Scheduler health check: Running with {metrics.get('total_cycles', 0)} cycles")
                        
        except Exception as e:
            logger.error(f"Error in scheduler monitoring: {e}")
            await asyncio.sleep(300)  # Wait 5 minutes before retrying if there's an error

@asynccontextmanager
async def lifespan(app: FastAPI):
    """Application lifespan manager with observability - OPTIMIZED FOR FAST STARTUP"""
    global scheduler_service
    
    # Startup - FAST PATH
    logger.info("üöÄ Starting PulseCheck API with AI-Optimized Observability")
    
    try:
        # ESSENTIAL STARTUP (fast operations only) - WITH ERROR HANDLING
        
        # Initialize observability system (with error handling)
        try:
            if observability_loaded:
                init_observability()
                logger.info("‚úÖ Observability system initialized")
            else:
                logger.warning("‚ö†Ô∏è Observability system not available, skipping initialization")
        except Exception as e:
            logger.warning(f"‚ö†Ô∏è Observability initialization failed: {e}")
        
        # Test database connection (fast check with error handling)
        try:
            if database_loaded:
                logger.info("‚úÖ Database connection module loaded")
            else:
                logger.warning("‚ö†Ô∏è Database system not available")
        except Exception as e:
            logger.warning(f"‚ö†Ô∏è Database connection check failed: {e}")
        
        # Validate configuration (fast check with error handling)
        try:
            if config_loaded:
                settings.validate_required_settings()
                logger.info("‚úÖ Configuration validated")
            else:
                logger.warning("‚ö†Ô∏è Configuration system not available, using minimal settings")
        except Exception as e:
            logger.warning(f"‚ö†Ô∏è Configuration validation failed: {e}")
        
        # System health check (fast check with error handling)
        try:
            if monitoring_loaded:
                health = monitor.check_system_health()
                logger.info(f"‚úÖ System health: {health.overall_status}")
            else:
                logger.warning("‚ö†Ô∏è Monitoring system not available, health check skipped")
        except Exception as e:
            logger.warning(f"‚ö†Ô∏è System health check failed: {e}")
        
        # Register routers (fast operation with error handling)
        try:
            logger.info("üîÑ Registering API routers...")
            register_routers()
            logger.info("‚úÖ All API routers registered successfully")
        except Exception as e:
            logger.error(f"‚ùå Router registration failed: {e}")
            # Continue without routers - health checks should still work
        
        # BACKGROUND TASK: Database warmup (heavy operation)
        if database_loaded:
            asyncio.create_task(_warmup_database_async())
        
        # BACKGROUND TASK: Start advanced scheduler (heavy operation)
        asyncio.create_task(_start_scheduler_async())
        
        # BACKGROUND TASK: Register comprehensive monitoring (heavy operation)
        asyncio.create_task(_register_comprehensive_monitoring_async())
        
        yield
        
    except Exception as e:
        logger.error(f"‚ùå Startup failed: {e}")
        # Don't crash the entire application - continue with minimal functionality
        logger.info("üîÑ Continuing with minimal functionality...")
        
        # Try to capture startup error for AI debugging
        try:
            if observability_loaded:
                observability.capture_error(e, {
                    "startup_phase": "application_initialization",
                    "critical": True
                }, severity="critical")
        except:
            pass
        
        # Continue with minimal functionality
        yield
        
    # Shutdown
    logger.info("üîÑ Shutting down PulseCheck API")
    
    try:
        # Generate final AI debugging summary
        try:
            if observability_loaded:
                summary = observability.get_ai_debugging_summary()
                logger.info(f"üìä Final system summary: {summary}")
        except Exception as e:
            logger.warning(f"‚ö†Ô∏è Failed to get AI debugging summary: {e}")
        
        if scheduler_service and scheduler_available:
            try:
                await scheduler_service.stop()
                logger.info("‚úÖ Scheduler stopped gracefully")
            except Exception as e:
                logger.error(f"Error stopping scheduler: {e}")
        
    except Exception as e:
        logger.error(f"Error during shutdown: {e}")

async def _warmup_database_async():
    """Background task for database warmup"""
    try:
        await asyncio.sleep(5)  # Wait a bit for main startup to complete
        db = get_database()
        await asyncio.to_thread(db.get_client().table('profiles').select("count", count="exact").execute)
        logger.info("‚úÖ Database connection pool warmed up")
    except Exception as e:
        logger.warning(f"‚ö†Ô∏è  Database warmup failed: {e}")

async def _start_scheduler_async():
    """Background task for scheduler startup"""
    try:
        await asyncio.sleep(10)  # Wait for main startup to complete
        
        if scheduler_available:
            logger.info("ü§ñ Starting advanced AI scheduler...")
            try:
                scheduler_service = get_scheduler_service()
                
                # Ensure scheduler is stopped first (clean state)
                try:
                    await scheduler_service.stop_scheduler()
                except:
                    pass
                    
                await asyncio.sleep(1)
                
                # Start the scheduler
                result = await scheduler_service.start_scheduler()
                
                if result.get("status") == "started":
                    logger.info("‚úÖ Advanced AI scheduler started successfully")
                    
                    # Enable testing mode by default for immediate responses
                    if os.getenv("ENABLE_TESTING_MODE", "true").lower() == "true":
                        try:
                            test_result = scheduler_service.proactive_ai.enable_testing_mode()
                            logger.info("üß™ Testing mode enabled: All AI timing delays bypassed")
                            logger.info(f"Testing mode status: {test_result}")
                        except Exception as e:
                            logger.error(f"Failed to enable testing mode: {e}")
                    
                    # Start background scheduler monitoring
                    monitor_task = asyncio.create_task(monitor_scheduler_health())
                    logger.info("üîç Scheduler health monitoring started")
                else:
                    logger.error(f"‚ö†Ô∏è Scheduler startup failed: {result}")
                    
                    # Still start monitoring to attempt recovery
                    monitor_task = asyncio.create_task(monitor_scheduler_health())
                    logger.info("üîç Scheduler health monitoring started (will attempt recovery)")
            except Exception as e:
                logger.error(f"‚ö†Ô∏è Scheduler startup failed (continuing without scheduler): {e}")
                # Continue without scheduler to allow Railway deployment to complete
                
                # Still start monitoring to attempt recovery later
                try:
                    monitor_task = asyncio.create_task(monitor_scheduler_health())
                    logger.info("üîç Scheduler health monitoring started (will attempt recovery)")
                except:
                    pass
        else:
            logger.info("‚ö†Ô∏è Advanced AI scheduler not available, continuing without background scheduling")
    except Exception as e:
        logger.error(f"Background scheduler startup failed: {e}")

async def _register_comprehensive_monitoring_async():
    """Background task for comprehensive monitoring registration"""
    try:
        await asyncio.sleep(15)  # Wait for main startup to complete
        
        # Import comprehensive monitoring routers
        try:
            from app.routers.comprehensive_monitoring import router as comprehensive_monitoring_router
            app.include_router(comprehensive_monitoring_router)
            logger.info("‚úÖ Comprehensive monitoring router registered")
            
            from app.routers.configuration_validation import router as config_validation_router
            app.include_router(config_validation_router)
            logger.info("‚úÖ Configuration validation router registered")
            
            from app.routers.predictive_monitoring import router as predictive_monitoring_router
            app.include_router(predictive_monitoring_router)
            logger.info("‚úÖ Predictive monitoring router registered")
            
            from app.routers.auto_resolution import router as auto_resolution_router
            app.include_router(auto_resolution_router)
            logger.info("‚úÖ Auto-resolution router registered")
            
        except ImportError as e:
            logger.warning(f"‚ö†Ô∏è  Some comprehensive monitoring features not available: {e}")
        except Exception as e:
            logger.error(f"‚ùå Failed to register comprehensive monitoring routers: {e}")
    except Exception as e:
        logger.error(f"Background comprehensive monitoring registration failed: {e}")

app = FastAPI(
    title="PulseCheck API",
    description="AI-powered wellness journal for tech workers",
    version="2.1.2",
    lifespan=lifespan,
    # Performance optimizations
    docs_url="/docs" if os.getenv("ENVIRONMENT") != "production" else None,
    redoc_url="/redoc" if os.getenv("ENVIRONMENT") != "production" else None,
    openapi_url="/openapi.json" if os.getenv("ENVIRONMENT") != "production" else None,
)

# Setup rate limiting if available and enabled
if limiter and config_loaded and settings.RATE_LIMIT_ENABLED:
    try:
        setup_rate_limiting(app)
        logger.info("Rate limiting enabled")
    except Exception as e:
        logger.warning(f"Failed to setup rate limiting: {e}")
else:
    if not settings.RATE_LIMIT_ENABLED:
        logger.info("Rate limiting disabled by configuration (RATE_LIMIT_ENABLED=false)")
    else:
        logger.warning("Rate limiting disabled - limiter not available")

# Performance middleware (order matters!)
# 1. GZip compression for response optimization
app.add_middleware(GZipMiddleware, minimum_size=1000)

# 2. CORS middleware with optimized settings
# Custom CORS configuration to handle Vercel preview deployments
class DynamicCORSMiddleware:
    def __init__(self, app: ASGIApp):
        self.app = app
        # Define allowed origin patterns
        self.allowed_patterns = [
            re.compile(r"^https://pulsecheck-mobile-app\.vercel\.app$"),
            re.compile(r"^https://pulsecheck-mobile-[a-z0-9]+-reitheaipms-projects\.vercel\.app$"),
            re.compile(r"^https://pulsecheck-mobile-[a-z0-9-]+-reitheaipms-projects\.vercel\.app$"),
            re.compile(r"^https://[a-z0-9-]+-reitheaipms-projects\.vercel\.app$"),
            re.compile(r"^http://localhost:\d+$"),
        ]
        self.exact_origins = {
            "https://pulsecheck-mobile-app.vercel.app",
            "https://pulse-check.vercel.app",
            "https://pulsecheck-web.vercel.app",
            "https://pulsecheck-app.vercel.app",
            "https://pulsecheck-mobile.vercel.app",
        }

    async def __call__(self, scope: Scope, receive: Receive, send: Send) -> None:
        if scope["type"] == "http":
            # BYPASS ALL CORS FOR HEALTH CHECKS
            if scope.get("path") in ["/health", "/health-fast", "/ready"]:
                await self.app(scope, receive, send)
                return
            
            headers = MutableHeaders(scope=scope)
            origin = headers.get("origin", "")
            
            # Check if origin is allowed
            is_allowed = origin in self.exact_origins or any(
                pattern.match(origin) for pattern in self.allowed_patterns
            )
            
            async def send_wrapper(message):
                if message["type"] == "http.response.start" and is_allowed:
                    headers = MutableHeaders(raw=message.get("headers", []))
                    headers["Access-Control-Allow-Origin"] = origin
                    headers["Access-Control-Allow-Credentials"] = "true"
                    headers["Access-Control-Allow-Methods"] = "GET, POST, PUT, PATCH, DELETE, OPTIONS"
                    headers["Access-Control-Allow-Headers"] = "*"
                    headers["Access-Control-Max-Age"] = "3600"
                    message["headers"] = headers.raw
                await send(message)
            
            # Handle preflight requests
            if scope["method"] == "OPTIONS" and is_allowed:
                response_headers = [
                    (b"access-control-allow-origin", origin.encode()),
                    (b"access-control-allow-credentials", b"true"),
                    (b"access-control-allow-methods", b"GET, POST, PUT, PATCH, DELETE, OPTIONS"),
                    (b"access-control-allow-headers", b"*"),
                    (b"access-control-max-age", b"3600"),
                    (b"content-length", b"0"),
                ]
                await send({
                    "type": "http.response.start",
                    "status": 200,
                    "headers": response_headers,
                })
                await send({
                    "type": "http.response.body",
                    "body": b"",
                })
                return
            
            await self.app(scope, receive, send_wrapper)
        else:
            await self.app(scope, receive, send)

# Apply custom CORS middleware
app.add_middleware(DynamicCORSMiddleware)

# 3. Security headers middleware
app.add_middleware(SecurityHeadersMiddleware)

# 4. Custom observability middleware for performance monitoring
app.add_middleware(ObservabilityMiddleware)

# Debug middleware disabled for production deployment
print("‚úÖ Debug middleware disabled (production mode)")
sys.stdout.flush()

# Security middleware - BYPASS HEALTH CHECKS
app.add_middleware(
    TrustedHostMiddleware,
    allowed_hosts=[
        "pulsecheck-mobile-app-production.up.railway.app",
        "spark-realm.vercel.app",
        "localhost:3000",
        "localhost:5173",
        "127.0.0.1:3000",
        "127.0.0.1:5173",
        "*"  # Allow all hosts for health checks
    ]
)

@app.middleware("http")
async def monitoring_middleware(request: Request, call_next):
    """Middleware for monitoring requests and responses - BYPASSES HEALTH CHECKS"""
    
    # BYPASS ALL MIDDLEWARE FOR HEALTH CHECKS
    if request.url.path in ["/health", "/health-fast", "/ready"]:
        return await call_next(request)
    
    start_time = time.time()
    
    try:
        # Process request
        response = await call_next(request)
        
        # Calculate response time
        response_time = (time.time() - start_time) * 1000
        
        # Add response time header (fast operation)
        response.headers["X-Response-Time"] = f"{response_time:.2f}ms"
        
        # BACKGROUND TASK: Log performance metric (heavy operation)
        asyncio.create_task(self._log_performance_async(
            response_time,
            request.method,
            request.url.path,
            response.status_code
        ))
        
        return response
        
    except Exception as e:
        # Calculate response time even for errors
        response_time = (time.time() - start_time) * 1000
        
        # BACKGROUND TASK: Log error with context (heavy operation)
        asyncio.create_task(self._log_error_async(
            e,
            request.method,
            request.url.path,
            response_time,
            request.headers.get("user-agent", ""),
            request.client.host if request.client else "unknown"
        ))
        
        # BACKGROUND TASK: Log performance metric for failed request (heavy operation)
        asyncio.create_task(self._log_performance_async(
            response_time,
            request.method,
            request.url.path,
            500,
            error=str(e)
        ))
        
        # Return error response (fast operation)
        return JSONResponse(
            status_code=500,
            content={
                "error": "Internal server error",
                "message": "An unexpected error occurred. Please try again later.",
                "timestamp": time.time()
            }
        )
    
    async def _log_performance_async(self, response_time: float, method: str, path: str, status_code: int, error: str = None):
        """Background task for performance logging"""
        try:
            log_performance(
                "api_response_time",
                response_time,
                "ms",
                {
                    "method": method,
                    "path": path,
                    "status_code": status_code,
                    **({"error": error} if error else {})
                }
            )
        except Exception as e:
            logger.warning(f"Background performance logging failed: {e}")
    
    async def _log_error_async(self, error: Exception, method: str, path: str, response_time: float, user_agent: str, client_ip: str):
        """Background task for error logging"""
        try:
            log_error(
                error,
                ErrorSeverity.HIGH,
                ErrorCategory.API_ENDPOINT,
                {
                    "method": method,
                    "path": path,
                    "response_time_ms": response_time,
                    "user_agent": user_agent,
                    "client_ip": client_ip
                },
                endpoint=f"{method} {path}"
            )
        except Exception as e:
            logger.warning(f"Background error logging failed: {e}")

@app.exception_handler(HTTPException)
async def http_exception_handler(request: Request, exc: HTTPException):
    """Handle HTTP exceptions with monitoring"""
    try:
        # Log the HTTP exception
        log_error(
            exc,
            ErrorSeverity.MEDIUM,
            ErrorCategory.API_ENDPOINT,
            {
                "method": request.method,
                "path": request.url.path,
                "status_code": exc.status_code,
                "detail": exc.detail
            },
            endpoint=f"{request.method} {request.url.path}"
        )
        
        return JSONResponse(
            status_code=exc.status_code,
            content={
                "error": "HTTP error",
                "message": exc.detail,
                "status_code": exc.status_code,
                "timestamp": time.time()
            }
        )
    except Exception as e:
        logger.error(f"Error in HTTP exception handler: {e}")
        return JSONResponse(
            status_code=500,
            content={
                "error": "Internal server error",
                "message": "An unexpected error occurred while handling the request.",
                "timestamp": time.time()
            }
        )

@app.exception_handler(Exception)
async def global_exception_handler(request: Request, exc: Exception):
    """
    Global exception handler with comprehensive AI debugging context
    """
    logger.error(f"Unhandled exception: {exc}", exc_info=True)
    
    # Capture with comprehensive context
    error_context = observability.capture_error(exc, {
        "handler": "global_exception_handler",
        "endpoint": str(request.url.path),
        "method": request.method,
        "unhandled": True
    }, severity="critical")
    
    # Return AI-friendly error response
    return JSONResponse(
        status_code=500,
        content={
            "error": True,
            "message": "An unexpected error occurred",
            "request_id": observability.get_current_request_id(),
            "timestamp": error_context["timestamp"],
            "ai_debugging": {
                "error_captured": True,
                "request_id": observability.get_current_request_id(),
                "debugging_endpoint": f"/api/v1/debug/error/{observability.get_current_request_id()}",
                "context_available": True
            },
            "support_info": {
                "message": "Please provide the request_id when contacting support",
                "contact": "Include this error ID in your support request"
            }
        },
        headers={
            "X-Request-ID": observability.get_current_request_id() or "no-request-id",
            "X-Error-Captured": "true"
        }
    )

# Simple CORS test endpoint
@app.get("/cors-test")
async def cors_test():
    """Simple endpoint to test CORS headers"""
    return {"message": "CORS test successful", "timestamp": time.time()}

# Health check endpoint - BYPASS ALL MIDDLEWARE
@app.get("/health")
async def health_check():
    """Ultra-fast health check for Railway deployment - BYPASSES ALL MIDDLEWARE"""
    return {
        "status": "healthy",
        "message": "PulseCheck API is running",
        "version": "2.1.2",
        "timestamp": time.time()
    }

# Additional ultra-fast health check that bypasses everything
@app.get("/health-fast")
async def health_check_fast():
    """Ultra-fast health check that bypasses all middleware and dependencies"""
    return {"status": "ok", "timestamp": time.time()}

# Comprehensive health check with detailed diagnostics
@app.get("/health-detailed")
async def health_check_detailed():
    """Comprehensive health check with detailed system diagnostics"""
    diagnostics = {
        "status": "checking",
        "timestamp": time.time(),
        "checks": {}
    }
    
    # 1. Python Runtime Check
    try:
        import sys
        diagnostics["checks"]["python_runtime"] = {
            "status": "healthy",
            "version": sys.version,
            "executable": sys.executable,
            "recursion_limit": sys.getrecursionlimit()
        }
    except Exception as e:
        diagnostics["checks"]["python_runtime"] = {
            "status": "unhealthy",
            "error": str(e)
        }
    
    # 2. Memory Check
    try:
        import psutil
        memory_info = psutil.virtual_memory()
        diagnostics["checks"]["memory"] = {
            "status": "healthy" if memory_info.percent < 90 else "warning",
            "available_gb": round(memory_info.available / (1024**3), 2),
            "usage_percent": memory_info.percent
        }
    except Exception as e:
        diagnostics["checks"]["memory"] = {
            "status": "unhealthy",
            "error": str(e)
        }
    
    # 3. Network Connectivity Check
    try:
        import socket
        import requests
        
        # DNS check
        try:
            socket.gethostbyname('google.com')
            dns_status = "healthy"
        except Exception as e:
            dns_status = "unhealthy"
            dns_error = str(e)
        
        # HTTP check
        try:
            response = requests.get("https://httpbin.org/get", timeout=5)
            http_status = "healthy"
            http_status_code = response.status_code
        except Exception as e:
            http_status = "unhealthy"
            http_error = str(e)
        
        diagnostics["checks"]["network"] = {
            "status": "healthy" if dns_status == "healthy" and http_status == "healthy" else "unhealthy",
            "dns": {"status": dns_status, "error": locals().get("dns_error", None)},
            "http": {"status": http_status, "status_code": locals().get("http_status_code", None), "error": locals().get("http_error", None)}
        }
    except Exception as e:
        diagnostics["checks"]["network"] = {
            "status": "unhealthy",
            "error": str(e)
        }
    
    # 4. Package Availability Check
    required_packages = [
        'fastapi', 'uvicorn', 'pydantic', 'supabase',
        'openai', 'python-dotenv', 'slowapi', 'requests'
    ]
    
    package_status = {}
    for package in required_packages:
        try:
            __import__(package)
            package_status[package] = "available"
        except ImportError:
            package_status[package] = "missing"
    
    diagnostics["checks"]["packages"] = {
        "status": "healthy" if all(status == "available" for status in package_status.values()) else "unhealthy",
        "packages": package_status
    }
    
    # 5. Environment Variables Check
    critical_env_vars = {
        "SUPABASE_URL": os.getenv("SUPABASE_URL"),
        "SUPABASE_ANON_KEY": os.getenv("SUPABASE_ANON_KEY"),
        "OPENAI_API_KEY": os.getenv("OPENAI_API_KEY"),
    }
    
    env_status = {}
    for var_name, var_value in critical_env_vars.items():
        env_status[var_name] = "present" if var_value else "missing"
    
    diagnostics["checks"]["environment"] = {
        "status": "healthy" if all(status == "present" for status in env_status.values()) else "unhealthy",
        "variables": env_status
    }
    
    # 6. Database Connection Check
    try:
        if database_loaded:
            db = get_database()
            # Quick connection test
            result = db.get_client().table('profiles').select('id').limit(1).execute()
            diagnostics["checks"]["database"] = {
                "status": "healthy",
                "connection": "successful"
            }
        else:
            diagnostics["checks"]["database"] = {
                "status": "unhealthy",
                "error": "Database system not loaded"
            }
    except Exception as e:
        diagnostics["checks"]["database"] = {
            "status": "unhealthy",
            "error": str(e)
        }
    
    # 7. Overall Status
    all_checks = diagnostics["checks"]
    healthy_checks = sum(1 for check in all_checks.values() if check.get("status") == "healthy")
    total_checks = len(all_checks)
    
    if healthy_checks == total_checks:
        diagnostics["status"] = "healthy"
        diagnostics["message"] = "All systems operational"
    elif healthy_checks >= total_checks * 0.8:  # 80% healthy
        diagnostics["status"] = "degraded"
        diagnostics["message"] = "Some systems degraded but operational"
    else:
        diagnostics["status"] = "unhealthy"
        diagnostics["message"] = "Multiple systems unhealthy"
    
    diagnostics["summary"] = {
        "total_checks": total_checks,
        "healthy_checks": healthy_checks,
        "unhealthy_checks": total_checks - healthy_checks
    }
    
    return diagnostics

# Monitoring endpoints
@app.get("/monitoring/errors")
async def get_error_summary(hours: int = 24):
    """Get error summary for monitoring"""
    try:
        return monitor.get_error_summary(hours)
    except Exception as e:
        log_error(e, ErrorSeverity.MEDIUM, ErrorCategory.API_ENDPOINT, {"endpoint": "error_summary"})
        return {"error": "Failed to get error summary"}

@app.get("/monitoring/performance")
async def get_performance_summary(hours: int = 24):
    """Get performance summary for monitoring"""
    try:
        return monitor.get_performance_summary(hours)
    except Exception as e:
        log_error(e, ErrorSeverity.MEDIUM, ErrorCategory.API_ENDPOINT, {"endpoint": "performance_summary"})
        return {"error": "Failed to get performance summary"}

@app.get("/monitoring/export")
async def export_monitoring_data():
    """Export monitoring data for analysis"""
    try:
        return monitor.export_data()
    except Exception as e:
        log_error(e, ErrorSeverity.MEDIUM, ErrorCategory.API_ENDPOINT, {"endpoint": "monitoring_export"})
        return {"error": "Failed to export monitoring data"}

@app.post("/monitoring/errors/{error_id}/resolve")
async def resolve_error(error_id: str, resolution_notes: str):
    """Mark an error as resolved"""
    try:
        success = monitor.resolve_error(error_id, resolution_notes)
        return {"success": success, "error_id": error_id}
    except Exception as e:
        log_error(e, ErrorSeverity.MEDIUM, ErrorCategory.API_ENDPOINT, {"endpoint": "resolve_error"})
        return {"error": "Failed to resolve error"}

# AI-Optimized Debugging Endpoints
@app.get("/ai-debug/error/{error_id}")
async def get_ai_debugging_context(error_id: str):
    """
    Get comprehensive debugging context for AI analysis
    This endpoint provides all the information an AI needs to debug an error
    """
    try:
        context = get_ai_debugging_context(error_id)
        return {
            "success": True,
            "error_id": error_id,
            "debugging_context": context,
            "ai_instructions": {
                "analysis_focus": [
                    "Review the error_details for the specific error",
                    "Check system_health for environmental issues",
                    "Analyze recent_performance for performance-related problems",
                    "Look at error_patterns for recurring issues",
                    "Follow debugging_recommendations for systematic approach"
                ],
                "debugging_approach": [
                    "1. Identify the root cause from potential_causes",
                    "2. Follow the suggested_solutions step by step",
                    "3. Use debugging_steps for systematic investigation",
                    "4. Check similar_errors for pattern recognition",
                    "5. Verify system_health and environment_vars"
                ]
            }
        }
    except Exception as e:
        log_error(e, ErrorSeverity.HIGH, ErrorCategory.API_ENDPOINT, {"endpoint": "ai_debug_context"})
        return {"error": "Failed to get AI debugging context"}

@app.get("/ai-debug/active-issues")
async def get_active_debugging_issues():
    """
    Get all active (unresolved) errors with AI debugging context
    """
    try:
        unresolved_errors = [e for e in monitor.errors if not e.resolved]
        
        active_issues = []
        for error in unresolved_errors[-20:]:  # Last 20 unresolved errors
            context = get_ai_debugging_context(error.error_id)
            active_issues.append({
                "error_id": error.error_id,
                "error_type": error.error_type,
                "severity": error.severity.value,
                "category": error.category.value,
                "timestamp": error.timestamp.isoformat(),
                "function_name": error.function_name,
                "file_path": error.file_path,
                "line_number": error.line_number,
                "potential_causes": error.potential_causes,
                "suggested_solutions": error.suggested_solutions,
                "debugging_context": context
            })
        
        return {
            "success": True,
            "total_active_issues": len(unresolved_errors),
            "recent_active_issues": active_issues,
            "ai_prioritization": {
                "critical_issues": [e for e in active_issues if e["severity"] == "critical"],
                "high_priority": [e for e in active_issues if e["severity"] == "high"],
                "medium_priority": [e for e in active_issues if e["severity"] == "medium"],
                "low_priority": [e for e in active_issues if e["severity"] == "low"]
            }
        }
    except Exception as e:
        log_error(e, ErrorSeverity.HIGH, ErrorCategory.API_ENDPOINT, {"endpoint": "active_debugging_issues"})
        return {"error": "Failed to get active debugging issues"}

@app.get("/ai-debug/error-patterns")
async def get_error_pattern_analysis():
    """
    Get comprehensive error pattern analysis for AI debugging
    """
    try:
        recent_errors = monitor.errors[-100:] if len(monitor.errors) > 100 else monitor.errors
        
        pattern_analysis = {
            "total_errors_analyzed": len(recent_errors),
            "time_period": "Last 100 errors",
            "patterns": monitor._get_error_pattern_summary(),
            "common_causes": monitor._get_common_causes(recent_errors),
            "recommended_actions": monitor._get_recommended_actions(recent_errors),
            "pattern_analysis": monitor._analyze_error_patterns(recent_errors),
            "ai_debugging_insights": {
                "most_critical_patterns": [],
                "recurring_issues": [],
                "environmental_factors": [],
                "performance_correlations": []
            }
        }
        
        # Analyze patterns for AI insights
        if recent_errors:
            # Find most critical patterns
            critical_errors = [e for e in recent_errors if e.severity == ErrorSeverity.CRITICAL]
            if critical_errors:
                pattern_analysis["ai_debugging_insights"]["most_critical_patterns"] = [
                    {
                        "error_type": e.error_type,
                        "category": e.category.value,
                        "common_causes": e.potential_causes[:3],
                        "frequency": len([err for err in critical_errors if err.error_type == e.error_type])
                    }
                    for e in critical_errors[:5]
                ]
            
            # Find recurring issues
            recurring_errors = [e for e in recent_errors if e.similar_errors]
            if recurring_errors:
                pattern_analysis["ai_debugging_insights"]["recurring_issues"] = [
                    {
                        "error_id": e.error_id,
                        "error_type": e.error_type,
                        "similar_errors_count": len(e.similar_errors),
                        "first_occurrence": e.timestamp.isoformat(),
                        "suggested_solutions": e.suggested_solutions[:3]
                    }
                    for e in recurring_errors[:5]
                ]
            
            # Environmental factors
            env_errors = [e for e in recent_errors if e.category in [ErrorCategory.CONFIGURATION, ErrorCategory.EXTERNAL_SERVICE]]
            if env_errors:
                pattern_analysis["ai_debugging_insights"]["environmental_factors"] = [
                    {
                        "error_type": e.error_type,
                        "environment_vars": e.environment_vars,
                        "system_info": e.system_info
                    }
                    for e in env_errors[:3]
                ]
        
        return {
            "success": True,
            "pattern_analysis": pattern_analysis,
            "ai_recommendations": {
                "immediate_focus": "Focus on critical and recurring errors first",
                "investigation_priority": "Check environmental factors and configuration issues",
                "prevention_strategy": "Implement monitoring for identified patterns"
            }
        }
    except Exception as e:
        log_error(e, ErrorSeverity.HIGH, ErrorCategory.API_ENDPOINT, {"endpoint": "error_pattern_analysis"})
        return {"error": "Failed to get error pattern analysis"}

@app.post("/ai-debug/attempt-resolution")
async def record_ai_debugging_attempt(error_id: str, attempt_details: Dict[str, Any]):
    """
    Record an AI debugging attempt for tracking and learning
    """
    try:
        # Find the error
        error = next((e for e in monitor.errors if e.error_id == error_id), None)
        if not error:
            return {"error": "Error not found"}
        
        # Record the attempt
        attempt_record = {
            "timestamp": datetime.now().isoformat(),
            "ai_model": attempt_details.get("ai_model", "unknown"),
            "approach": attempt_details.get("approach", "unknown"),
            "analysis": attempt_details.get("analysis", ""),
            "solution_attempted": attempt_details.get("solution_attempted", ""),
            "success": attempt_details.get("success", False),
            "notes": attempt_details.get("notes", "")
        }
        
        if not error.ai_debugging_attempts:
            error.ai_debugging_attempts = []
        
        error.ai_debugging_attempts.append(attempt_record)
        
        return {
            "success": True,
            "error_id": error_id,
            "attempt_recorded": attempt_record,
            "total_attempts": len(error.ai_debugging_attempts)
        }
    except Exception as e:
        log_error(e, ErrorSeverity.MEDIUM, ErrorCategory.API_ENDPOINT, {"endpoint": "record_ai_attempt"})
        return {"error": "Failed to record AI debugging attempt"}

# Note: OPTIONS requests are handled by CustomCORSMiddleware above
# No need for additional OPTIONS handler as it causes conflicts

def register_routers():
    """Simplified router registration with comprehensive error handling"""
    routers_registered = 0
    routers_failed = 0
    
    print("üîÑ Starting simplified router registration...")
    sys.stdout.flush()
    
    # Add /app to Python path for imports
    sys.path.insert(0, '/app')
    print("üîÑ Added /app to Python path")
    sys.stdout.flush()
    
    # Core routers (required) - with individual error handling
    core_routers = [
        ("auth", "app.routers.auth", "auth"),
        ("journal", "app.routers.journal", "journal"),
        ("adaptive_ai", "app.routers.adaptive_ai", "adaptive-ai"),
        ("checkins", "app.routers.checkins", "checkins"),
        ("monitoring", "app.routers.monitoring", "monitoring"),
    ]
    
    # Register core routers with individual error handling
    for router_name, import_path, tag in core_routers:
        print(f"üîÑ Importing {router_name} router...")
        sys.stdout.flush()
        try:
            # Try to import the module
            module = __import__(import_path, fromlist=['router'])
            
            # Try to get the router
            try:
                router = getattr(module, 'router')
            except AttributeError:
                print(f"‚ùå {router_name} router not found in module")
                routers_failed += 1
                sys.stdout.flush()
                continue
            
            # Try to include the router
            try:
                app.include_router(router, prefix=f"/api/v1/{tag}", tags=[tag])
                print(f"‚úÖ {router_name} router registered")
                routers_registered += 1
                sys.stdout.flush()
            except Exception as e:
                print(f"‚ùå {router_name} router registration failed: {e}")
                routers_failed += 1
                sys.stdout.flush()
                
        except ImportError as e:
            print(f"‚ùå {router_name} router import failed: {e}")
            routers_failed += 1
            sys.stdout.flush()
        except Exception as e:
            print(f"‚ùå {router_name} router failed: {e}")
            routers_failed += 1
            sys.stdout.flush()
    
    # Optional routers (non-critical) - with individual error handling
    optional_routers = [
        ("proactive_ai", "app.routers.proactive_ai", "proactive-ai"),
        ("admin", "app.routers.admin", "admin"),
        ("admin_monitoring", "app.routers.admin_monitoring", "admin-monitoring"),
        ("manual_ai_response", "app.routers.manual_ai_response", "manual-ai"),
        ("webhook_handler", "app.routers.webhook_handler", "webhook"),
    ]
    
    # Register optional routers with individual error handling
    for router_name, import_path, tag in optional_routers:
        print(f"üîÑ Importing optional {router_name} router...")
        sys.stdout.flush()
        try:
            # Try to import the module
            module = __import__(import_path, fromlist=['router'])
            
            # Try to get the router
            try:
                router = getattr(module, 'router')
            except AttributeError:
                print(f"‚ö†Ô∏è {router_name} router not found in module (optional)")
                routers_failed += 1
                sys.stdout.flush()
                continue
            
            # Try to include the router
            try:
                app.include_router(router, prefix=f"/api/v1/{tag}", tags=[tag])
                print(f"‚úÖ {router_name} router registered")
                routers_registered += 1
                sys.stdout.flush()
            except Exception as e:
                print(f"‚ö†Ô∏è {router_name} router registration failed (optional): {e}")
                routers_failed += 1
                sys.stdout.flush()
                
        except ImportError as e:
            print(f"‚ö†Ô∏è {router_name} router import failed (optional): {e}")
            routers_failed += 1
            sys.stdout.flush()
        except Exception as e:
            print(f"‚ö†Ô∏è {router_name} router failed (optional): {e}")
            routers_failed += 1
            sys.stdout.flush()
    
    print(f"üéâ Router registration complete! {routers_registered} routers registered, {routers_failed} failed")
    sys.stdout.flush()

@app.get("/")
async def root():
    return {"message": "PulseCheck API is running", "status": "healthy", "version": "2.1.2"}

@app.get("/ready")
async def readiness_check():
    """Readiness check for Railway deployment"""
    return {"status": "ready", "message": "Application is ready to serve requests"}

# QUICK FIX: Manual AI endpoints added directly to main.py to bypass router registration issues
@app.get("/api/v1/manual-ai/debug-database/{user_id}")
async def debug_database_access(user_id: str):
    """Debug database access to see why journal entries aren't visible"""
    try:
        from app.core.database import get_database
        
        db = get_database()
        # Use service role client to bypass RLS
        supabase = db.get_service_client()
        
        # Test 1: Raw table query
        try:
            raw_response = supabase.table("journal_entries").select("*").execute()
            raw_count = len(raw_response.data) if raw_response.data else 0
        except Exception as e:
            raw_count = f"ERROR: {str(e)}"
        
        # Test 2: User-specific query
        try:
            user_response = supabase.table("journal_entries").select("*").eq("user_id", user_id).execute()
            user_count = len(user_response.data) if user_response.data else 0
            user_data = user_response.data[:3] if user_response.data else []  # First 3 entries
        except Exception as e:
            user_count = f"ERROR: {str(e)}"
            user_data = []
        
        # Test 3: Check if using service role
        client_info = str(type(supabase))
        
        return {
            "user_id": user_id,
            "diagnosis": "Journal entries exist in mobile app but API can't see them",
            "database_tests": {
                "raw_table_count": raw_count,
                "user_specific_count": user_count,
                "user_entries_sample": user_data,
                "client_type": client_info
            },
            "possible_causes": [
                "RLS policies blocking service role access",
                "Mobile app using different database",
                "Service role vs anon role mismatch",
                "Table schema differences"
            ],
            "next_steps": "Check RLS policies and service role permissions"
        }
        
    except Exception as e:
        return {
            "error": f"Database test failed: {str(e)}",
            "user_id": user_id,
            "diagnosis": "Cannot access database at all"
        }

@app.get("/api/v1/manual-ai/list-journals/{user_id}")
async def list_user_journals(user_id: str):
    """List journal entries for a user so they can see journal IDs for testing"""
    try:
        from app.core.database import get_database
        
        db = get_database()
        # Use service role client to bypass RLS
        supabase = db.get_service_client()
        
        # Get recent journal entries for this user
        response = supabase.table("journal_entries").select(
            "id, content, mood_level, energy_level, created_at"
        ).eq("user_id", user_id).order("created_at", desc=True).limit(10).execute()
        
        if not response.data:
            return {
                "user_id": user_id,
                "message": "No journal entries found for this user",
                "journal_entries": [],
                "next_step": "Create a journal entry first using your mobile app"
            }
        
        # Format the results for easy reading
        formatted_entries = []
        for entry in response.data:
            formatted_entries.append({
                "journal_id": entry["id"],
                "content_preview": entry["content"][:100] + "..." if len(entry["content"]) > 100 else entry["content"],
                "mood_level": entry["mood_level"],
                "energy_level": entry["energy_level"],
                "created_at": entry["created_at"]
            })
        
        return {
            "success": True,
            "user_id": user_id,
            "total_entries": len(formatted_entries),
            "journal_entries": formatted_entries,
            "instructions": {
                "you_asked": "I don't know what the journal ID is?",
                "answer": "Here are your journal IDs! Pick any journal_id from the list above for testing.",
                "next_step": "Use any journal_id with other endpoints or simply note them for reference"
            }
        }
        
    except Exception as e:
        return {
            "error": f"Failed to list journal entries: {str(e)}",
            "user_id": user_id,
            "troubleshooting": "Check database connectivity and user_id format"
        }

@app.post("/api/v1/manual-ai/respond-to-latest/{user_id}")
async def manual_respond_to_latest(user_id: str):
    """Automatically find user's most recent journal entry and generate AI response"""
    try:
        from app.core.database import get_database
        import uuid
        from datetime import datetime, timezone
        
        db = get_database()
        # Use service role client to bypass RLS
        supabase = db.get_service_client()
        
        # Find the most recent journal entry for this user
        response = supabase.table("journal_entries").select(
            "id, content, mood_level, energy_level, created_at"
        ).eq("user_id", user_id).order("created_at", desc=True).limit(1).execute()
        
        if not response.data:
            return {
                "error": "No journal entries found for this user",
                "user_id": user_id,
                "next_step": "Create a journal entry first using your mobile app"
            }
        
        latest_entry = response.data[0]
        journal_id = latest_entry["id"]
        
        # Check if AI response already exists in ai_insights table
        existing_insight = supabase.table("ai_insights").select("id").eq("journal_entry_id", journal_id).execute()
        
        if existing_insight.data:
            return {
                "message": "AI response already exists for this journal entry",
                "journal_id": journal_id,
                "journal_preview": latest_entry["content"][:100] + "...",
                "existing_insight_id": existing_insight.data[0]["id"],
                "suggestion": f"Use respond-to-journal/{journal_id} to regenerate"
            }
        
        # Generate simple AI response (inline implementation)
        ai_response = f"Thank you for sharing your journal entry! I can see you wrote: '{latest_entry['content'][:50]}...'. This is a test AI response generated at {datetime.now(timezone.utc).isoformat()}."
        
        # Insert AI response into ai_insights table
        ai_insight_data = {
            "id": str(uuid.uuid4()),
            "journal_entry_id": journal_id,
            "user_id": user_id,
            "ai_response": ai_response,
            "persona_used": "test_persona",
            "confidence_score": 0.85,
            "created_at": datetime.now(timezone.utc).isoformat()
        }
        
        insight_response = supabase.table("ai_insights").insert(ai_insight_data).execute()
        ai_insight = insight_response.data[0] if insight_response.data else ai_insight_data
        
        return {
            "success": True,
            "message": "AI response generated for latest journal entry",
            "journal_id": journal_id,
            "journal_created": latest_entry["created_at"],
            "journal_preview": latest_entry["content"][:100] + "...",
            "ai_insight_id": ai_insight.get("id"),
            "ai_response_preview": ai_insight.get("ai_response", "")[:100] + "...",
            "monitoring_check": f"GET /api/v1/ai-monitoring/last-action/{user_id}"
        }
        
    except Exception as e:
        logger.error(f"Error in manual AI response for user {user_id}: {e}")
        return {
            "error": f"Failed to generate AI response: {str(e)}",
            "user_id": user_id,
            "troubleshooting": "Check logs and database connectivity"
        }

@app.get("/api/v1/frontend-fix/ai-responses/{user_id}")
async def get_ai_responses_for_frontend(user_id: str):
    """
    Frontend-friendly endpoint to get AI responses without authentication
    This fixes the 404 errors the frontend is experiencing
    """
    try:
        from app.core.database import get_database
        
        db = get_database()
        supabase = db.get_service_client()  # Use service role to bypass RLS
        
        # Get user's recent journal entries with AI responses
        journal_result = supabase.table("journal_entries").select(
            "id, content, created_at, energy_level"
        ).eq("user_id", user_id).order("created_at", desc=True).limit(10).execute()
        
        if not journal_result.data:
            return {
                "user_id": user_id,
                "journal_entries": [],
                "ai_responses": [],
                "message": "No journal entries found"
            }
        
        # Get AI insights for these journal entries (only table that exists)
        journal_ids = [entry["id"] for entry in journal_result.data]
        ai_result = supabase.table("ai_insights").select(
            "id, journal_entry_id, ai_response, persona_used, confidence_score, created_at"
        ).eq("user_id", user_id).in_("journal_entry_id", journal_ids).order("created_at", desc=True).execute()
        
        # Format AI responses
        ai_responses = []
        
        # Add AI insights
        for insight in (ai_result.data or []):
            ai_responses.append({
                "id": insight["id"],
                "journal_entry_id": insight["journal_entry_id"],
                "response": insight["ai_response"],
                "persona": insight["persona_used"],
                "confidence": insight["confidence_score"],
                "created_at": insight["created_at"],
                "source": "ai_insights"
            })
        
        # Sort by creation time
        ai_responses.sort(key=lambda x: x["created_at"], reverse=True)
        
        return {
            "success": True,
            "user_id": user_id,
            "total_journal_entries": len(journal_result.data),
            "total_ai_responses": len(ai_responses),
            "journal_entries": journal_result.data,
            "ai_responses": ai_responses,
            "latest_ai_response": ai_responses[0] if ai_responses else None,
            "note": "This endpoint bypasses authentication for frontend testing"
        }
        
    except Exception as e:
        logger.error(f"Error getting AI responses for frontend: {e}")
        return {
            "error": f"Failed to get AI responses: {str(e)}",
            "user_id": user_id,
            "troubleshooting": "Check database connectivity and user_id format"
        }

# Signal handler for graceful shutdown
def signal_handler(signum, frame):
    """Handle shutdown signals gracefully"""
    global scheduler_service
    
    logger.info(f"üõë Received signal {signum}, initiating graceful shutdown...")
    
    if scheduler_service and scheduler_available:
        try:
            # Stop the scheduler gracefully
            asyncio.create_task(scheduler_service.stop_scheduler())
            logger.info("‚úÖ Scheduler shutdown initiated")
        except Exception as e:
            logger.error(f"Error during scheduler shutdown: {e}")
    
    # Exit gracefully
    sys.exit(0)

# Register signal handlers for graceful shutdown
signal.signal(signal.SIGTERM, signal_handler)  # Railway sends SIGTERM
signal.signal(signal.SIGINT, signal_handler)   # Ctrl+C during development

@app.get("/api/v1/debug/test-database-access")
async def test_database_access():
    """Simple test to check if we can access journal entries after RLS fix"""
    try:
        from app.core.database import get_database
        
        db = get_database()
        supabase = db.get_client()
        
        # Test 1: Try to read ANY journal entries (should work with service role)
        try:
            result = supabase.table("journal_entries").select("id, user_id, content, created_at").limit(5).execute()
            journal_count = len(result.data) if result.data else 0
            sample_entries = result.data[:2] if result.data else []
        except Exception as e:
            journal_count = f"ERROR: {str(e)}"
            sample_entries = []
        
        # Test 2: Check if table exists and what columns it has
        try:
            table_info = supabase.rpc("get_table_info", {"table_name": "journal_entries"}).execute()
            table_exists = True
        except Exception as e:
            table_exists = f"ERROR: {str(e)}"
            table_info = None
        
        return {
            "timestamp": datetime.now(timezone.utc).isoformat(),
            "test_type": "direct_database_access",
            "rls_fix_applied": "2025-07-01T23:02:48",
            "journal_table_access": {
                "total_entries_found": journal_count,
                "sample_entries": sample_entries,
                "table_exists": table_exists
            },
            "database_client": str(type(supabase)),
            "success": journal_count > 0
        }
    
    except Exception as e:
        return {
            "timestamp": datetime.now(timezone.utc).isoformat(),
            "test_type": "direct_database_access",
            "error": str(e),
            "success": False
        }

@app.post("/api/v1/debug/enable-premium-unlimited-ai/{user_id}")
async def enable_premium_unlimited_ai(user_id: str):
    """
    Enable premium unlimited AI responses for testing account - disables all fallback responses
    This bypasses cost limits and fallback responses for premium testing
    """
    try:
        from app.core.database import get_database
        
        db = get_database()
        supabase = db.get_service_client()  # Use service role to bypass RLS
        
        # Update user profile to PREMIUM tier with HIGH AI interaction
        profile_update = {
            "ai_interaction_level": "HIGH",  # Maximum AI interactions
            "updated_at": datetime.now(timezone.utc).isoformat()
        }
        
        # Check if profile exists, create if not
        existing_profile = supabase.table("user_ai_preferences").select("*").eq("user_id", user_id).execute()
        
        if existing_profile.data:
            # Update existing profile
            supabase.table("user_ai_preferences").update(profile_update).eq("user_id", user_id).execute()
        else:
            # Create new profile
            profile_update.update({
                "user_id": user_id,
                "created_at": datetime.now(timezone.utc).isoformat()
            })
            supabase.table("user_ai_preferences").insert(profile_update).execute()
        
        # Add to testing users list in comprehensive AI service
        from app.services.comprehensive_proactive_ai_service import get_comprehensive_ai_service
        comp_ai_service = get_comprehensive_ai_service()
        if user_id not in comp_ai_service.testing_user_ids:
            comp_ai_service.testing_user_ids.add(user_id)
        
        # Enable testing mode for immediate responses
        comp_ai_service.enable_testing_mode()
        
        return {
            "success": True,
            "message": f"Premium unlimited AI enabled for user {user_id}",
            "configuration": {
                "tier": "PREMIUM_TESTING",
                "ai_interaction_level": "HIGH",
                "fallback_responses": "DISABLED",
                "cost_limits": "BYPASSED",
                "multi_persona": "ENABLED",
                "immediate_responses": "ENABLED",
                "daily_limit": "UNLIMITED"
            },
            "features_enabled": [
                "Multiple AI personas (Pulse, Sage, Spark, Anchor)",
                "No fallback responses - only real AI responses",
                "Immediate response timing (no delays)",
                "Unlimited daily AI interactions",
                "Cost limit bypass",
                "Premium AI models (GPT-4o)"
            ],
            "next_steps": [
                "Create a new journal entry",
                "Check for immediate AI responses from multiple personas",
                "Test reply functionality with real AI responses"
            ]
        }
        
    except Exception as e:
        logger.error(f"Error enabling premium unlimited AI for user {user_id}: {e}")
        return {
            "error": f"Failed to enable premium unlimited AI: {str(e)}",
            "user_id": user_id,
            "troubleshooting": "Check logs and database connectivity"
        }

@app.post("/api/v1/debug/disable-fallback-responses/{user_id}")
async def disable_fallback_responses(user_id: str):
    """
    Disable fallback responses specifically for a user account
    Forces the system to always use real AI responses, never fallbacks
    """
    try:
        from app.core.database import get_database
        
        db = get_database()
        supabase = db.get_service_client()
        
        # Create or update a special configuration to mark this user as no-fallback
        no_fallback_config = {
            "user_id": user_id,
            "setting_name": "disable_fallback_responses",
            "setting_value": "true",
            "created_at": datetime.now(timezone.utc).isoformat(),
            "description": "Disables all fallback responses for premium testing account"
        }
        
        # Store in user_ai_preferences or create a custom settings table entry
        try:
            # Try to update existing preference
            existing = supabase.table("user_ai_preferences").select("*").eq("user_id", user_id).execute()
            
            if existing.data:
                # Update existing with no-fallback flag
                supabase.table("user_ai_preferences").update({
                    "ai_interaction_level": "HIGH",
                    "fallback_disabled": True,
                    "premium_override": True,
                    "updated_at": datetime.now(timezone.utc).isoformat()
                }).eq("user_id", user_id).execute()
            else:
                # Create new preference with no-fallback settings
                supabase.table("user_ai_preferences").insert({
                    "user_id": user_id,
                    "ai_interaction_level": "HIGH",
                    "fallback_disabled": True,
                    "premium_override": True,
                    "created_at": datetime.now(timezone.utc).isoformat(),
                    "updated_at": datetime.now(timezone.utc).isoformat()
                }).execute()
                
        except Exception as db_error:
            logger.warning(f"Could not update user_ai_preferences: {db_error}")
            # Fallback to storing in a comment or metadata
        
        return {
            "success": True,
            "user_id": user_id,
            "fallback_responses": "DISABLED",
            "configuration": {
                "ai_interaction_level": "HIGH",
                "fallback_disabled": True,
                "premium_override": True,
                "cost_limits_bypassed": True
            },
            "message": "Fallback responses disabled - all responses will be real AI generated content",
            "note": "This ensures premium experience with no generic fallback messages"
        }
        
    except Exception as e:
        logger.error(f"Error disabling fallback responses for user {user_id}: {e}")
        return {
            "error": f"Failed to disable fallback responses: {str(e)}",
            "user_id": user_id
        }

@app.get("/api/v1/scheduler/health")
async def scheduler_comprehensive_health():
    """
    Comprehensive scheduler health check with detailed diagnostics
    """
    global scheduler_service
    
    health_info = {
        "timestamp": datetime.now(timezone.utc).isoformat(),
        "scheduler_available": scheduler_available,
        "scheduler_service_exists": scheduler_service is not None,
        "testing_mode_env": os.getenv("ENABLE_TESTING_MODE", "true"),
        "auto_start_env": os.getenv("AUTO_START_SCHEDULER", "true"),
        "environment": os.getenv("ENVIRONMENT", "unknown")
    }
    
    if scheduler_service and scheduler_available:
        try:
            status = scheduler_service.get_scheduler_status()
            health_info["scheduler_status"] = status
            health_info["is_healthy"] = status.get("running", False) and status.get("status") == "running"
            
            # Check testing mode
            try:
                testing_status = scheduler_service.proactive_ai.get_testing_mode_status()
                health_info["testing_mode"] = testing_status
            except:
                health_info["testing_mode"] = {"error": "Could not get testing mode status"}
                
        except Exception as e:
            health_info["error"] = str(e)
            health_info["is_healthy"] = False
    else:
        health_info["is_healthy"] = False
        health_info["error"] = "Scheduler service not initialized"
    
    # Add recommendations
    if not health_info.get("is_healthy"):
        health_info["recommendations"] = [
            "1. Check Railway logs for startup errors",
            "2. Ensure all environment variables are set",
            "3. Try manual restart: POST /api/v1/scheduler/start",
            "4. Check if testing mode is enabled",
            "5. Monitor will attempt auto-recovery every minute"
        ]
    
    return health_info

if __name__ == "__main__":
    import uvicorn
    # Use Railway's PORT environment variable if available
    port = int(os.getenv('PORT', getattr(settings, 'port', 8000)))
    uvicorn.run(app, host=getattr(settings, 'host', '0.0.0.0'), port=port) 