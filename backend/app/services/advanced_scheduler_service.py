"""
Advanced Scheduler Service

Sophisticated background task scheduler for proactive AI engagement with:
- Multiple timing strategies (immediate, spaced, pattern-based)
- Real-time monitoring and analytics
- A/B testing framework
- Resource optimization
- Error handling and recovery
- Performance tracking

This orchestrates the entire proactive AI ecosystem.
"""

import logging
import asyncio
from datetime import datetime, timezone, timedelta
from typing import Dict, Any, List, Optional
from dataclasses import dataclass, asdict
from enum import Enum
import json
import traceback

from apscheduler.schedulers.asyncio import AsyncIOScheduler
from apscheduler.triggers.interval import IntervalTrigger
from apscheduler.triggers.cron import CronTrigger

from ..core.database import Database, get_database
from .comprehensive_proactive_ai_service import ComprehensiveProactiveAIService
from .adaptive_ai_service import AdaptiveAIService

logger = logging.getLogger(__name__)

class SchedulerStatus(Enum):
    STARTING = "starting"
    RUNNING = "running"
    PAUSED = "paused"
    STOPPED = "stopped"
    ERROR = "error"

@dataclass
class SchedulerMetrics:
    """Real-time scheduler performance metrics"""
    total_cycles: int
    successful_cycles: int
    failed_cycles: int
    avg_cycle_duration_seconds: float
    avg_users_processed_per_cycle: int
    avg_opportunities_per_cycle: int
    avg_engagements_per_cycle: int
    last_cycle_timestamp: Optional[datetime]
    next_cycle_timestamp: Optional[datetime]
    current_status: SchedulerStatus
    uptime_hours: float
    error_rate: float
    engagement_success_rate: float

@dataclass
class CycleResult:
    """Result from a single scheduler cycle"""
    cycle_id: str
    timestamp: datetime
    duration_seconds: float
    users_processed: int
    opportunities_found: int
    engagements_executed: int
    errors: List[str]
    status: str

class AdvancedSchedulerService:
    """Advanced scheduler for comprehensive proactive AI system"""
    
    def __init__(self, db: Database):
        self.db = db
        self.scheduler = AsyncIOScheduler()
        self.status = SchedulerStatus.STOPPED
        self.start_time: Optional[datetime] = None
        
        # Initialize comprehensive AI service
        self.proactive_ai = ComprehensiveProactiveAIService(db)
        
        # Performance tracking
        self.metrics = SchedulerMetrics(
            total_cycles=0,
            successful_cycles=0,
            failed_cycles=0,
            avg_cycle_duration_seconds=0.0,
            avg_users_processed_per_cycle=0,
            avg_opportunities_per_cycle=0,
            avg_engagements_per_cycle=0,
            last_cycle_timestamp=None,
            next_cycle_timestamp=None,
            current_status=SchedulerStatus.STOPPED,
            uptime_hours=0.0,
            error_rate=0.0,
            engagement_success_rate=0.0
        )
        
        # Cycle history for analytics
        self.cycle_history: List[CycleResult] = []
        self.max_history_size = 100
        
        # Configuration
        self.config = {
            "main_cycle_interval_minutes": 5,      # Main cycle every 5 minutes
            "immediate_cycle_interval_minutes": 1, # Immediate responses every 1 minute
            "analytics_cycle_interval_minutes": 15, # Analytics update every 15 minutes
            "cleanup_cycle_interval_hours": 24,    # Cleanup old data daily
            "max_users_per_cycle": 50,             # Process max 50 users per cycle
            "enable_a_b_testing": True,            # Enable A/B testing
            "enable_performance_optimization": True # Enable performance optimization
        }
    
    async def start_scheduler(self) -> Dict[str, Any]:
        """Start the advanced scheduler with all job types"""
        try:
            if self.status == SchedulerStatus.RUNNING:
                return {"status": "already_running", "message": "Scheduler is already running"}
            
            self.status = SchedulerStatus.STARTING
            self.start_time = datetime.now(timezone.utc)
            
            # Add main proactive AI cycle (every 5 minutes)
            self.scheduler.add_job(
                self._main_proactive_cycle,
                trigger=IntervalTrigger(minutes=self.config["main_cycle_interval_minutes"]),
                id="main_proactive_cycle",
                name="Main Proactive AI Engagement Cycle",
                max_instances=1,
                coalesce=True
            )
            
            # Add immediate response cycle (every 1 minute for high-engagement users)
            self.scheduler.add_job(
                self._immediate_response_cycle,
                trigger=IntervalTrigger(minutes=self.config["immediate_cycle_interval_minutes"]),
                id="immediate_response_cycle", 
                name="Immediate Response Cycle",
                max_instances=1,
                coalesce=True
            )
            
            # Add analytics and monitoring cycle (every 15 minutes)
            self.scheduler.add_job(
                self._analytics_cycle,
                trigger=IntervalTrigger(minutes=self.config["analytics_cycle_interval_minutes"]),
                id="analytics_cycle",
                name="Analytics and Monitoring Cycle",
                max_instances=1,
                coalesce=True
            )
            
            # Add daily cleanup cycle (every 24 hours at 2 AM)
            self.scheduler.add_job(
                self._cleanup_cycle,
                trigger=CronTrigger(hour=2, minute=0),
                id="cleanup_cycle",
                name="Daily Cleanup Cycle",
                max_instances=1,
                coalesce=True
            )
            
            # Start the scheduler
            self.scheduler.start()
            self.status = SchedulerStatus.RUNNING
            
            logger.info("‚úÖ Advanced Scheduler started successfully with all job types")
            
            return {
                "status": "started",
                "message": "Advanced scheduler started successfully",
                "jobs": [
                    {
                        "id": job.id,
                        "name": job.name,
                        "next_run": job.next_run_time.isoformat() if job.next_run_time else None
                    }
                    for job in self.scheduler.get_jobs()
                ],
                "config": self.config
            }
            
        except Exception as e:
            self.status = SchedulerStatus.ERROR
            logger.error(f"Failed to start advanced scheduler: {e}")
            logger.error(traceback.format_exc())
            return {
                "status": "error",
                "message": f"Failed to start scheduler: {str(e)}",
                "error": str(e)
            }
    
    async def stop_scheduler(self) -> Dict[str, Any]:
        """Stop the advanced scheduler"""
        try:
            if self.status == SchedulerStatus.STOPPED:
                return {"status": "already_stopped", "message": "Scheduler is already stopped"}
            
            self.scheduler.shutdown(wait=True)
            self.status = SchedulerStatus.STOPPED
            
            logger.info("‚úÖ Advanced Scheduler stopped successfully")
            
            return {
                "status": "stopped",
                "message": "Advanced scheduler stopped successfully",
                "final_metrics": asdict(self.metrics)
            }
            
        except Exception as e:
            logger.error(f"Error stopping scheduler: {e}")
            return {
                "status": "error",
                "message": f"Error stopping scheduler: {str(e)}",
                "error": str(e)
            }
    
    async def _main_proactive_cycle(self):
        """Main proactive AI engagement cycle - runs every 5 minutes
        ‚úÖ RE-ENABLED: Conversation threading issues fixed"""
        cycle_id = f"main_{datetime.now(timezone.utc).strftime('%Y%m%d_%H%M%S')}"
        start_time = datetime.now(timezone.utc)
        
        try:
            logger.info(f"üîÑ Main proactive cycle {cycle_id} - RUNNING")
            
            # ‚úÖ RE-ENABLED: Comprehensive engagement cycle now safe to run
            # Conversation threading issues have been fixed
            result = await self.proactive_ai.run_comprehensive_engagement_cycle()
            
            # Fallback if no result returned
            if not result:
                result = {
                    "active_users": 0,
                    "opportunities_found": 0,
                    "engagements_executed": 0,
                    "status": "no_active_users"
                }
            
            # Calculate duration
            duration = (datetime.now(timezone.utc) - start_time).total_seconds()
            
            # Create cycle result
            cycle_result = CycleResult(
                cycle_id=cycle_id,
                timestamp=start_time,
                duration_seconds=duration,
                users_processed=result.get("active_users", 0),
                opportunities_found=result.get("opportunities_found", 0),
                engagements_executed=result.get("engagements_executed", 0),
                errors=[],
                status=result.get("status", "unknown")
            )
            
            # Update metrics
            await self._update_metrics(cycle_result)
            
            # Store cycle history
            self._store_cycle_result(cycle_result)
            
            logger.info(f"‚úÖ Main proactive cycle completed: {cycle_id} - DISABLED (preventing duplicates)")
            
        except Exception as e:
            logger.error(f"‚ùå Error in main proactive cycle {cycle_id}: {e}")
            logger.error(traceback.format_exc())
            
            # Record failed cycle
            duration = (datetime.now(timezone.utc) - start_time).total_seconds()
            cycle_result = CycleResult(
                cycle_id=cycle_id,
                timestamp=start_time,
                duration_seconds=duration,
                users_processed=0,
                opportunities_found=0,
                engagements_executed=0,
                errors=[str(e)],
                status="error"
            )
            
            await self._update_metrics(cycle_result)
            self._store_cycle_result(cycle_result)
    
    async def _immediate_response_cycle(self):
        """Immediate response cycle for high-engagement users - runs every 1 minute
        ‚úÖ RE-ENABLED: Conversation threading issues fixed"""
        cycle_id = f"immediate_{datetime.now(timezone.utc).strftime('%Y%m%d_%H%M%S')}"
        start_time = datetime.now(timezone.utc)
        
        try:
            logger.debug(f"üöÄ Starting immediate response cycle: {cycle_id}")
            
            # Get users who are actively engaging (recently interacted with AI)
            active_engagement_users = await self._get_actively_engaging_users()
            
            if not active_engagement_users:
                logger.debug(f"No actively engaging users found for immediate cycle: {cycle_id}")
                return
            
            total_executed = 0
            
            # Process immediate opportunities for actively engaging users
            for user_id in active_engagement_users[:10]:  # Limit to 10 users per immediate cycle
                try:
                    opportunities = await self.proactive_ai.check_comprehensive_opportunities(user_id)
                    
                    # Look for immediate opportunities (delay <= 2 minutes)
                    immediate_opportunities = [
                        opp for opp in opportunities 
                        if opp.get("delay_minutes", 0) <= 2
                    ]
                    
                    for opportunity in immediate_opportunities[:1]:  # Max 1 per user
                        success = await self.proactive_ai.execute_comprehensive_engagement(user_id, opportunity)
                        if success:
                            total_executed += 1
                
                except Exception as e:
                    logger.error(f"Error processing immediate opportunity for user {user_id}: {e}")
                    continue
            
            duration = (datetime.now(timezone.utc) - start_time).total_seconds()
            
            if total_executed > 0:
                logger.info(f"‚úÖ Immediate response cycle completed: {cycle_id} - {total_executed} immediate engagements")
            
        except Exception as e:
            logger.error(f"‚ùå Error in immediate response cycle {cycle_id}: {e}")
    
    async def _analytics_cycle(self):
        """Analytics and monitoring cycle - runs every 15 minutes"""
        cycle_id = f"analytics_{datetime.now(timezone.utc).strftime('%Y%m%d_%H%M%S')}"
        
        try:
            logger.debug(f"üìä Starting analytics cycle: {cycle_id}")
            
            # Update scheduler metrics
            await self._calculate_comprehensive_metrics()
            
            # Perform A/B testing analysis
            if self.config["enable_a_b_testing"]:
                await self._analyze_ab_testing_results()
            
            # Optimize performance settings
            if self.config["enable_performance_optimization"]:
                await self._optimize_performance_settings()
            
            # Store analytics to database
            await self._store_analytics_snapshot()
            
            logger.debug(f"‚úÖ Analytics cycle completed: {cycle_id}")
            
        except Exception as e:
            logger.error(f"‚ùå Error in analytics cycle {cycle_id}: {e}")
    
    async def _cleanup_cycle(self):
        """Daily cleanup cycle - runs every 24 hours"""
        cycle_id = f"cleanup_{datetime.now(timezone.utc).strftime('%Y%m%d_%H%M%S')}"
        
        try:
            logger.info(f"üßπ Starting cleanup cycle: {cycle_id}")
            
            # Clean up old cycle history
            if len(self.cycle_history) > self.max_history_size:
                self.cycle_history = self.cycle_history[-self.max_history_size:]
            
            # Clean up old analytics data from database
            await self._cleanup_old_analytics_data()
            
            # Reset daily metrics
            await self._reset_daily_metrics()
            
            logger.info(f"‚úÖ Cleanup cycle completed: {cycle_id}")
            
        except Exception as e:
            logger.error(f"‚ùå Error in cleanup cycle {cycle_id}: {e}")
    
    async def _get_actively_engaging_users(self) -> List[str]:
        """Get users who are actively engaging (for immediate responses)"""
        try:
            # CRITICAL: Use service role client to bypass RLS for AI operations
            client = self.db.get_service_client()
            
            # Users who interacted with AI in last 10 minutes
            cutoff_time = (datetime.now(timezone.utc) - timedelta(minutes=10)).isoformat()
            
            # TODO: Query user engagement events table when implemented
            # For now, get users with recent AI insights
            result = client.table("ai_insights").select("user_id").gte("created_at", cutoff_time).execute()
            
            active_users = list(set(insight["user_id"] for insight in (result.data or [])))
            
            return active_users
            
        except Exception as e:
            logger.error(f"Error getting actively engaging users: {e}")
            return []
    
    async def _update_metrics(self, cycle_result: CycleResult):
        """Update scheduler metrics with cycle result"""
        self.metrics.total_cycles += 1
        
        if cycle_result.status in ["success", "no_active_users", "disabled_temporarily"]:
            self.metrics.successful_cycles += 1
        else:
            self.metrics.failed_cycles += 1
        
        # Update averages
        self.metrics.avg_cycle_duration_seconds = (
            (self.metrics.avg_cycle_duration_seconds * (self.metrics.total_cycles - 1) + cycle_result.duration_seconds) 
            / self.metrics.total_cycles
        )
        
        self.metrics.avg_users_processed_per_cycle = (
            (self.metrics.avg_users_processed_per_cycle * (self.metrics.total_cycles - 1) + cycle_result.users_processed)
            / self.metrics.total_cycles
        )
        
        self.metrics.avg_opportunities_per_cycle = (
            (self.metrics.avg_opportunities_per_cycle * (self.metrics.total_cycles - 1) + cycle_result.opportunities_found)
            / self.metrics.total_cycles
        )
        
        self.metrics.avg_engagements_per_cycle = (
            (self.metrics.avg_engagements_per_cycle * (self.metrics.total_cycles - 1) + cycle_result.engagements_executed)
            / self.metrics.total_cycles
        )
        
        # Update timestamps
        self.metrics.last_cycle_timestamp = cycle_result.timestamp
        next_job = self.scheduler.get_job("main_proactive_cycle")
        self.metrics.next_cycle_timestamp = next_job.next_run_time if next_job else None
        
        # Update status
        self.metrics.current_status = self.status
        
        # Calculate uptime
        if self.start_time:
            self.metrics.uptime_hours = (datetime.now(timezone.utc) - self.start_time).total_seconds() / 3600
        
        # Calculate error rate
        self.metrics.error_rate = (self.metrics.failed_cycles / self.metrics.total_cycles) * 100 if self.metrics.total_cycles > 0 else 0
        
        # Calculate engagement success rate
        total_opportunities = self.metrics.avg_opportunities_per_cycle * self.metrics.total_cycles
        total_engagements = self.metrics.avg_engagements_per_cycle * self.metrics.total_cycles
        self.metrics.engagement_success_rate = (total_engagements / total_opportunities) * 100 if total_opportunities > 0 else 0
    
    def _store_cycle_result(self, cycle_result: CycleResult):
        """Store cycle result in history"""
        self.cycle_history.append(cycle_result)
        
        # Maintain history size limit
        if len(self.cycle_history) > self.max_history_size:
            self.cycle_history = self.cycle_history[-self.max_history_size:]
    
    async def _calculate_comprehensive_metrics(self):
        """Calculate comprehensive metrics from recent data"""
        # This would analyze recent performance and update metrics
        pass
    
    async def _analyze_ab_testing_results(self):
        """Analyze A/B testing results and optimize strategies"""
        # This would analyze which engagement strategies are most effective
        pass
    
    async def _optimize_performance_settings(self):
        """Optimize performance settings based on recent data"""
        # This would adjust timing and frequency based on performance
        pass
    
    async def _store_analytics_snapshot(self):
        """Store current analytics snapshot to database"""
        try:
            # CRITICAL: Use service role client to bypass RLS for AI operations
            client = self.db.get_service_client()
            
            analytics_data = {
                "id": str(__import__('uuid').uuid4()),
                "timestamp": datetime.now(timezone.utc).isoformat(),
                "metrics": asdict(self.metrics),
                "config": self.config,
                "recent_cycles": [asdict(cycle) for cycle in self.cycle_history[-10:]]  # Last 10 cycles
            }
            
            # TODO: Create scheduler_analytics table
            # result = client.table("scheduler_analytics").insert(analytics_data).execute()
            
        except Exception as e:
            logger.error(f"Error storing analytics snapshot: {e}")
    
    async def _cleanup_old_analytics_data(self):
        """Clean up old analytics data"""
        try:
            # Clean up analytics data older than 30 days
            cutoff_date = (datetime.now(timezone.utc) - timedelta(days=30)).isoformat()
            
            # TODO: Implement cleanup when scheduler_analytics table exists
            # client = self.db.get_client()
            # result = client.table("scheduler_analytics").delete().lt("timestamp", cutoff_date).execute()
            
        except Exception as e:
            logger.error(f"Error cleaning up old analytics data: {e}")
    
    async def _reset_daily_metrics(self):
        """Reset daily metrics for fresh tracking"""
        # Reset certain metrics that should be calculated daily
        pass
    
    def get_scheduler_status(self) -> Dict[str, Any]:
        """Get current scheduler status and metrics"""
        return {
            "status": self.status.value,
            "running": self.scheduler.running if hasattr(self.scheduler, 'running') else False,
            "metrics": asdict(self.metrics),
            "jobs": [
                {
                    "id": job.id,
                    "name": job.name,
                    "next_run": job.next_run_time.isoformat() if job.next_run_time else None,
                    "func": str(job.func)
                }
                for job in self.scheduler.get_jobs()
            ] if self.scheduler else [],
            "recent_cycles": [asdict(cycle) for cycle in self.cycle_history[-5:]],  # Last 5 cycles
            "config": self.config
        }
    
    def get_performance_analytics(self) -> Dict[str, Any]:
        """Get detailed performance analytics"""
        return {
            "metrics": asdict(self.metrics),
            "cycle_history": [asdict(cycle) for cycle in self.cycle_history],
            "performance_trends": self._calculate_performance_trends(),
            "optimization_recommendations": self._get_optimization_recommendations()
        }
    
    def _calculate_performance_trends(self) -> Dict[str, Any]:
        """Calculate performance trends from cycle history"""
        if len(self.cycle_history) < 2:
            return {"status": "insufficient_data"}
        
        recent_cycles = self.cycle_history[-10:]  # Last 10 cycles
        
        # Calculate trends
        avg_duration = sum(cycle.duration_seconds for cycle in recent_cycles) / len(recent_cycles)
        avg_engagements = sum(cycle.engagements_executed for cycle in recent_cycles) / len(recent_cycles)
        success_rate = (sum(1 for cycle in recent_cycles if cycle.status == "success") / len(recent_cycles)) * 100
        
        return {
            "avg_duration_seconds": avg_duration,
            "avg_engagements_per_cycle": avg_engagements,
            "success_rate_percent": success_rate,
            "total_recent_cycles": len(recent_cycles)
        }
    
    def _get_optimization_recommendations(self) -> List[str]:
        """Get optimization recommendations based on performance"""
        recommendations = []
        
        if self.metrics.error_rate > 10:
            recommendations.append("High error rate detected - consider increasing error handling")
        
        if self.metrics.avg_cycle_duration_seconds > 30:
            recommendations.append("Cycles taking too long - consider optimizing database queries")
        
        if self.metrics.engagement_success_rate < 20:
            recommendations.append("Low engagement success rate - consider adjusting timing strategies")
        
        return recommendations


# Global scheduler instance
_scheduler_instance: Optional[AdvancedSchedulerService] = None


def get_scheduler_service() -> AdvancedSchedulerService:
    """Get or create scheduler service instance"""
    global _scheduler_instance
    
    if _scheduler_instance is None:
        db = get_database()
        _scheduler_instance = AdvancedSchedulerService(db)
        logger.info("Created new AdvancedSchedulerService instance")
    
    return _scheduler_instance


async def start():
    """Start the scheduler service"""
    scheduler = get_scheduler_service()
    return await scheduler.start_scheduler()


async def stop():
    """Stop the scheduler service"""
    scheduler = get_scheduler_service()
    return await scheduler.stop_scheduler()


def get_status():
    """Get scheduler status"""
    scheduler = get_scheduler_service()
    return scheduler.get_scheduler_status() 